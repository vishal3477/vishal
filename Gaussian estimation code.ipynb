{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian estimate conditional PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import warnings\n",
    "from sklearn import model_selection\n",
    "import gensim \n",
    "\n",
    "\n",
    "fil_data=pd.read_excel('filtered_data.xlsx')\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(fil_data[['text']], \n",
    "                                                                            fil_data['category'], test_size=0.33)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=100, window=1, min_count=1, workers=4,dm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)  # you can continue training with the loaded model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no=np.zeros((87144,500))\n",
    "\n",
    "for i in range(87144):\n",
    "    X_train_no[i,:]= model.infer_vector(np.asarray(X_train)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train);\n",
    "Y_test = np.array(Y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_trainno=np.zeros((Y_train.size))\n",
    "for i in range(Y_train.size):\n",
    "    if Y_train[i]==\"BUSINESS\":\n",
    "        Y_trainno[i]=1\n",
    "    elif Y_train[i]==\"COMEDY\":\n",
    "        Y_trainno[i]=2\n",
    "    elif Y_train[i]==\"ENTERTAINMENT\":\n",
    "        Y_trainno[i]=3\n",
    "    elif Y_train[i]==\"FOOD & DRINK\":\n",
    "        Y_trainno[i]=4\n",
    "    elif Y_train[i]==\"HEALTHY LIVING\":\n",
    "        Y_trainno[i]=5\n",
    "    elif Y_train[i]==\"PARENTING\":\n",
    "        Y_trainno[i]=6\n",
    "    elif Y_train[i]==\"POLITICS\":\n",
    "        Y_trainno[i]=7\n",
    "    elif Y_train[i]==\"QUEER VOICES\":\n",
    "        Y_trainno[i]=8\n",
    "    elif Y_train[i]==\"SPORTS\":\n",
    "        Y_trainno[i]=9\n",
    "    elif Y_train[i]==\"STYLE & BEAUTY\":\n",
    "        Y_trainno[i]=10\n",
    "    elif Y_train[i]==\"TRAVEL\":\n",
    "        Y_trainno[i]=11\n",
    "    elif Y_train[i]==\"WELLNESS\":\n",
    "        Y_trainno[i]=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train=[]\n",
    "x2_train=[]\n",
    "x3_train=[]\n",
    "x4_train=[]\n",
    "x5_train=[]\n",
    "x6_train=[]\n",
    "x7_train=[]\n",
    "x8_train=[]\n",
    "x9_train=[]\n",
    "x10_train=[]\n",
    "x11_train=[]\n",
    "x12_train=[]\n",
    "x1_test=[]\n",
    "x2_test=[]\n",
    "x3_test=[]\n",
    "x4_test=[]\n",
    "x5_test=[]\n",
    "x6_test=[]\n",
    "x7_test=[]\n",
    "x8_test=[]\n",
    "x9_test=[]\n",
    "x10_test=[]\n",
    "x11_test=[]\n",
    "x12_test=[]\n",
    "y1_train=[]\n",
    "y2_train=[]\n",
    "y3_train=[]\n",
    "y4_train=[]\n",
    "y5_train=[]\n",
    "y6_train=[]\n",
    "y7_train=[]\n",
    "y8_train=[]\n",
    "y9_train=[]\n",
    "y10_train=[]\n",
    "y11_train=[]\n",
    "y12_train=[]\n",
    "y1_test=[]\n",
    "y2_test=[]\n",
    "y3_test=[]\n",
    "y4_test=[]\n",
    "y5_test=[]\n",
    "y6_test=[]\n",
    "y7_test=[]\n",
    "y8_test=[]\n",
    "y9_test=[]\n",
    "y10_test=[]\n",
    "y11_test=[]\n",
    "y12_test=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Y_train.size):\n",
    "    if Y_train[i]==\"BUSINESS\":\n",
    "        x1_train.append(X_train_no[i,:])\n",
    "        y1_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"COMEDY\":\n",
    "        x2_train.append(X_train_no[i,:])\n",
    "        y2_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"ENTERTAINMENT\":\n",
    "        x3_train.append(X_train_no[i,:])\n",
    "        y3_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"FOOD & DRINK\":\n",
    "        x4_train.append(X_train_no[i,:])\n",
    "        y4_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"HEALTHY LIVING\":\n",
    "        x5_train.append(X_train_no[i,:])\n",
    "        y5_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"PARENTING\":\n",
    "        x6_train.append(X_train_no[i,:])\n",
    "        y6_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"POLITICS\":\n",
    "        x7_train.append(X_train_no[i,:])\n",
    "        y7_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"QUEER VOICES\":\n",
    "        x8_train.append(X_train_no[i,:])\n",
    "        y8_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"SPORTS\":\n",
    "        x9_train.append(X_train_no[i,:])\n",
    "        y9_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"STYLE & BEAUTY\":\n",
    "        x10_train.append(X_train_no[i,:])\n",
    "        y10_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"TRAVEL\":\n",
    "        x11_train.append(X_train_no[i,:])\n",
    "        y11_train.append(Y_trainno[i])\n",
    "    elif Y_train[i]==\"WELLNESS\":\n",
    "        x12_train.append(X_train_no[i,:])\n",
    "        y12_train.append(Y_trainno[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testno=np.zeros((Y_test.size))\n",
    "for i in range(Y_test.size):\n",
    "    if Y_test[i]==\"BUSINESS\":\n",
    "        Y_testno[i]=1\n",
    "    elif Y_test[i]==\"COMEDY\":\n",
    "        Y_testno[i]=2\n",
    "    elif Y_test[i]==\"ENTERTAINMENT\":\n",
    "        Y_testno[i]=3\n",
    "    elif Y_test[i]==\"FOOD & DRINK\":\n",
    "        Y_testno[i]=4\n",
    "    elif Y_test[i]==\"HEALTHY LIVING\":\n",
    "        Y_testno[i]=5\n",
    "    elif Y_test[i]==\"PARENTING\":\n",
    "        Y_testno[i]=6\n",
    "    elif Y_test[i]==\"POLITICS\":\n",
    "        Y_testno[i]=7\n",
    "    elif Y_test[i]==\"QUEER VOICES\":\n",
    "        Y_testno[i]=8\n",
    "    elif Y_test[i]==\"SPORTS\":\n",
    "        Y_testno[i]=9\n",
    "    elif Y_test[i]==\"STYLE & BEAUTY\":\n",
    "        Y_testno[i]=10\n",
    "    elif Y_test[i]==\"TRAVEL\":\n",
    "        Y_testno[i]=11\n",
    "    elif Y_test[i]==\"WELLNESS\":\n",
    "        Y_testno[i]=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=len(x1_train)+len(x2_train)+len(x3_train)+len(x4_train)+len(x5_train)+len(x6_train)+len(x7_train)+len(x8_train)+len(x9_train)+len(x10_train)+len(x11_train)+len(x12_train)\n",
    "print(total)\n",
    "total=len(y1_train)+len(y2_train)+len(y3_train)+len(y4_train)+len(y5_train)+len(y6_train)+len(y7_train)+len(y8_train)+len(y9_train)+len(y10_train)+len(y11_train)+len(y12_train)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu1_es=(1/len(x1_train))*(np.sum(x1_train,axis=0));   \n",
    "mu2_es=(1/len(x2_train))*(np.sum(x2_train,axis=0));\n",
    "mu3_es=(1/len(x3_train))*(np.sum(x3_train,axis=0));\n",
    "mu4_es=(1/len(x4_train))*(np.sum(x4_train,axis=0));   \n",
    "mu5_es=(1/len(x5_train))*(np.sum(x5_train,axis=0));\n",
    "mu6_es=(1/len(x6_train))*(np.sum(x6_train,axis=0));\n",
    "mu7_es=(1/len(x7_train))*(np.sum(x7_train,axis=0));   \n",
    "mu8_es=(1/len(x8_train))*(np.sum(x8_train,axis=0));\n",
    "mu9_es=(1/len(x9_train))*(np.sum(x9_train,axis=0));\n",
    "mu10_es=(1/len(x10_train))*(np.sum(x10_train,axis=0));   \n",
    "mu11_es=(1/len(x11_train))*(np.sum(x11_train,axis=0));\n",
    "mu12_es=(1/len(x12_train))*(np.sum(x12_train,axis=0));\n",
    "\n",
    "#Estimated variance\n",
    "cov1_es=(1/len(x1_train))*np.dot((np.transpose(x1_train-mu1_es)),(x1_train-mu1_es));\n",
    "cov2_es=(1/len(x2_train))*np.dot((np.transpose(x2_train-mu2_es)),(x2_train-mu2_es));\n",
    "cov3_es=(1/len(x3_train))*np.dot((np.transpose(x3_train-mu3_es)),(x3_train-mu3_es));\n",
    "cov4_es=(1/len(x4_train))*np.dot((np.transpose(x4_train-mu4_es)),(x4_train-mu4_es));\n",
    "cov5_es=(1/len(x5_train))*np.dot((np.transpose(x5_train-mu5_es)),(x5_train-mu5_es));\n",
    "cov6_es=(1/len(x6_train))*np.dot((np.transpose(x6_train-mu6_es)),(x6_train-mu6_es));\n",
    "cov7_es=(1/len(x7_train))*np.dot((np.transpose(x7_train-mu7_es)),(x7_train-mu7_es));\n",
    "cov8_es=(1/len(x8_train))*np.dot((np.transpose(x8_train-mu8_es)),(x8_train-mu8_es));\n",
    "cov9_es=(1/len(x9_train))*np.dot((np.transpose(x9_train-mu9_es)),(x9_train-mu9_es));\n",
    "cov10_es=(1/len(x10_train))*np.dot((np.transpose(x10_train-mu10_es)),(x10_train-mu10_es));\n",
    "cov11_es=(1/len(x11_train))*np.dot((np.transpose(x11_train-mu11_es)),(x11_train-mu11_es));\n",
    "cov12_es=(1/len(x12_train))*np.dot((np.transpose(x12_train-mu12_es)),(x12_train-mu12_es));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_no=np.zeros((42923,500))\n",
    "\n",
    "for i in range(42923):\n",
    "    X_test_no[i,:]= model.infer_vector(np.asarray(X_test)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_prior=np.zeros((12))\n",
    "prob_prior[0]=len(x1_train)/total\n",
    "prob_prior[1]=len(x2_train)/total\n",
    "prob_prior[2]=len(x3_train)/total\n",
    "prob_prior[3]=len(x4_train)/total\n",
    "prob_prior[4]=len(x5_train)/total\n",
    "prob_prior[5]=len(x6_train)/total\n",
    "prob_prior[6]=len(x7_train)/total\n",
    "prob_prior[7]=len(x8_train)/total\n",
    "prob_prior[8]=len(x9_train)/total\n",
    "prob_prior[9]=len(x10_train)/total\n",
    "prob_prior[10]=len(x11_train)/total\n",
    "prob_prior[11]=len(x12_train)/total\n",
    "print(prob_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from scipy.stats import multivariate_normal\n",
    "y_pred=np.zeros((42923))\n",
    "for i in range(42923):\n",
    "    p1=multivariate_normal.pdf(X_test_no[i,:],mu1_es,cov1_es)*prob_prior[0]\n",
    "    p2=multivariate_normal.pdf(X_test_no[i,:],mu2_es,cov2_es)*prob_prior[1]\n",
    "    p3=multivariate_normal.pdf(X_test_no[i,:],mu3_es,cov3_es)*prob_prior[2]\n",
    "    p4=multivariate_normal.pdf(X_test_no[i,:],mu4_es,cov4_es)*prob_prior[3]\n",
    "    p5=multivariate_normal.pdf(X_test_no[i,:],mu5_es,cov5_es)*prob_prior[4]\n",
    "    p6=multivariate_normal.pdf(X_test_no[i,:],mu6_es,cov6_es)*prob_prior[5]\n",
    "    p7=multivariate_normal.pdf(X_test_no[i,:],mu7_es,cov7_es)*prob_prior[6]\n",
    "    p8=multivariate_normal.pdf(X_test_no[i,:],mu8_es,cov8_es)*prob_prior[7]\n",
    "    p9=multivariate_normal.pdf(X_test_no[i,:],mu9_es,cov9_es)*prob_prior[8]\n",
    "    p10=multivariate_normal.pdf(X_test_no[i,:],mu10_es,cov10_es)*prob_prior[9]\n",
    "    p11=multivariate_normal.pdf(X_test_no[i,:],mu11_es,cov11_es)*prob_prior[10]\n",
    "    p12=multivariate_normal.pdf(X_test_no[i,:],mu12_es,cov12_es)*prob_prior[11]\n",
    "    prob=np.column_stack((p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12))\n",
    "    print(prob)\n",
    "    try:\n",
    "        y_pred[i]=(np.where(prob[0,:] == np.amax(prob[0,:]))[0])+1\n",
    "    except:\n",
    "        y_pred[i]=1\n",
    "print(confusion_matrix(Y_testno, y_pred))\n",
    "print(accuracy_score(Y_testno, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(Y_testno, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
