{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import warnings\n",
    "fil_data=pd.read_excel('filtered_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(fil_data[['text']], \n",
    "                                                                            fil_data['category'], test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import sklearn.model_selection\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(fil_data[['text']], \n",
    "                                                                            fil_data['category'], test_size=0.33)\n",
    "\n",
    "X_train = np.array(X_train);\n",
    "X_test = np.array(X_test);\n",
    "Y_train = np.array(Y_train);\n",
    "Y_test = np.array(Y_test);\n",
    "\n",
    "procText_train = [] \n",
    "procText_test = [] \n",
    "number_train = len(X_train) \n",
    "number_test = len(X_test) \n",
    "\n",
    "lemmetizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "def get_words(headlines_list):\n",
    "    headlines = headlines_list[0]   \n",
    "    headlines_only_letters = re.sub('[^a-zA-Z]', ' ', headlines)\n",
    "    words = nltk.word_tokenize(headlines_only_letters.lower())\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [lemmetizer.lemmatize(w) for w in words if w not in stops]\n",
    "    return ' '.join(meaningful_words )\n",
    "\n",
    "for i in range(number_train):\n",
    "    proctext = get_words(X_train[i]) #Processing the data and getting words with no special characters, numbers or html tags\n",
    "    procText_train.append( proctext )\n",
    "print(\"train words done\")\n",
    "for i in range(number_test):\n",
    "    proctext = get_words(X_test[i]) #Processing the data and getting words with no special characters, numbers or html tags\n",
    "    procText_test.append( proctext )\n",
    "print(\"test words done\")    \n",
    "vectorize = sklearn.feature_extraction.text.TfidfVectorizer(analyzer = \"word\", max_features=10000)\n",
    "tfidwords_train = vectorize.fit_transform(procText_train)\n",
    "X_train = tfidwords_train.toarray()\n",
    "\n",
    "tfidwords_test = vectorize.transform(procText_test)\n",
    "X_test = tfidwords_test.toarray()\n",
    "print(\"vectorizer done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train=[]\n",
    "x2_train=[]\n",
    "x3_train=[]\n",
    "x4_train=[]\n",
    "x5_train=[]\n",
    "x6_train=[]\n",
    "x7_train=[]\n",
    "x8_train=[]\n",
    "x9_train=[]\n",
    "x10_train=[]\n",
    "x11_train=[]\n",
    "x12_train=[]\n",
    "x1_test=[]\n",
    "x2_test=[]\n",
    "x3_test=[]\n",
    "x4_test=[]\n",
    "x5_test=[]\n",
    "x6_test=[]\n",
    "x7_test=[]\n",
    "x8_test=[]\n",
    "x9_test=[]\n",
    "x10_test=[]\n",
    "x11_test=[]\n",
    "x12_test=[]\n",
    "y1_train=[]\n",
    "y2_train=[]\n",
    "y3_train=[]\n",
    "y4_train=[]\n",
    "y5_train=[]\n",
    "y6_train=[]\n",
    "y7_train=[]\n",
    "y8_train=[]\n",
    "y9_train=[]\n",
    "y10_train=[]\n",
    "y11_train=[]\n",
    "y12_train=[]\n",
    "y1_test=[]\n",
    "y2_test=[]\n",
    "y3_test=[]\n",
    "y4_test=[]\n",
    "y5_test=[]\n",
    "y6_test=[]\n",
    "y7_test=[]\n",
    "y8_test=[]\n",
    "y9_test=[]\n",
    "y10_test=[]\n",
    "y11_test=[]\n",
    "y12_test=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Y_train.size):\n",
    "    if Y_train[i]==\"BUSINESS\":\n",
    "        x1_train.append(X_train[i,:])\n",
    "        y1_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"COMEDY\":\n",
    "        x2_train.append(X_train[i,:])\n",
    "        y2_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"ENTERTAINMENT\":\n",
    "        x3_train.append(X_train[i,:])\n",
    "        y3_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"FOOD & DRINK\":\n",
    "        x4_train.append(X_train[i,:])\n",
    "        y4_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"HEALTHY LIVING\":\n",
    "        x5_train.append(X_train[i,:])\n",
    "        y5_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"PARENTING\":\n",
    "        print(2)\n",
    "        x6_train.append(X_train[i,:])\n",
    "        y6_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"POLITICS\":\n",
    "        print(1)\n",
    "        x7_train.append(X_train[i,:])\n",
    "        y7_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"QUEER VOICES\":\n",
    "        x8_train.append(X_train[i,:])\n",
    "        y8_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"SPORTS\":\n",
    "        x9_train.append(X_train[i,:])\n",
    "        y9_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"STYLE & BEAUTY\":\n",
    "        x10_train.append(X_train[i,:])\n",
    "        y10_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"TRAVEL\":\n",
    "        x11_train.append(X_train[i,:])\n",
    "        y11_train.append(Y_train[i])\n",
    "    elif Y_train[i]==\"WELLNESS\":\n",
    "        x12_train.append(X_train[i,:])\n",
    "        y12_train.append(Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Y_test.size):\n",
    "    if Y_test[i]==\"BUSINESS\":\n",
    "        x1_test.append(X_test[i,:])\n",
    "        y1_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"COMEDY\":\n",
    "        x2_test.append(X_test[i,:])\n",
    "        y2_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"ENTERTAINMENT\":\n",
    "        x3_test.append(X_test[i,:])\n",
    "        y3_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"FOOD & DRINK\":\n",
    "        x4_test.append(X_test[i,:])\n",
    "        y4_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"HEALTHY LIVING\":\n",
    "        x5_test.append(X_test[i,:])\n",
    "        y5_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"PARENTING\":\n",
    "        x6_test.append(X_test[i,:])\n",
    "        y6_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"POLITICS\":\n",
    "        x7_test.append(X_test[i,:])\n",
    "        y7_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"QUEER VOICES\":\n",
    "        x8_test.append(X_test[i,:])\n",
    "        y8_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"SPORTS\":\n",
    "        x9_test.append(X_test[i,:])\n",
    "        y9_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"STYLE & BEAUTY\":\n",
    "        x10_test.append(X_test[i,:])\n",
    "        y10_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"TRAVEL\":\n",
    "        x11_test.append(X_test[i,:])\n",
    "        y11_test.append(Y_test[i])\n",
    "    elif Y_test[i]==\"WELLNESS\":\n",
    "        x12_test.append(X_test[i,:])\n",
    "        y12_test.append(Y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=len(x1_train)+len(x2_train)+len(x3_train)+len(x4_train)+len(x5_train)+len(x6_train)+len(x7_train)+len(x8_train)+len(x9_train)+len(x10_train)+len(x11_train)+len(x12_train)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x1_train))\n",
    "print(len(x2_train))\n",
    "print(len(x3_train))\n",
    "print(len(x4_train))\n",
    "print(len(x5_train))\n",
    "print(len(x6_train))\n",
    "print(len(x7_train))\n",
    "print(len(x8_train))\n",
    "print(len(x9_train))\n",
    "print(len(x10_train))\n",
    "print(len(x11_train))\n",
    "print(len(x12_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train=np.array(x1_train)\n",
    "x2_train=np.array(x2_train)\n",
    "x3_train=np.array(x3_train)\n",
    "x4_train=np.array(x4_train)\n",
    "x5_train=np.array(x5_train)\n",
    "x6_train=np.array(x6_train)\n",
    "x7_train=np.array(x7_train)\n",
    "x8_train=np.array(x8_train)\n",
    "x9_train=np.array(x9_train)\n",
    "x10_train=np.array(x10_train)\n",
    "x11_train=np.array(x11_train)\n",
    "x12_train=np.array(x12_train)\n",
    "x1_test=np.array(x1_test)\n",
    "x2_test=np.array(x2_test)\n",
    "x3_test=np.array(x3_test)\n",
    "x4_test=np.array(x4_test)\n",
    "x5_test=np.array(x5_test)\n",
    "x6_test=np.array(x6_test)\n",
    "x7_test=np.array(x7_test)\n",
    "x8_test=np.array(x8_test)\n",
    "x9_test=np.array(x9_test)\n",
    "x10_test=np.array(x10_test)\n",
    "x11_test=np.array(x11_test)\n",
    "x12_test=np.array(x12_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words=np.zeros((10000))\n",
    "for i in range(x1_train.shape[1]):\n",
    "    for j in range(X_train.shape[0]):\n",
    "        if X_train[j,i]!=0:\n",
    "            count_words[i]+=1;\n",
    "print(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=np.zeros((12,10000))\n",
    "for i in range(x1_train.shape[1]):\n",
    "    count=0\n",
    "    for j in range(x1_train.shape[0]):\n",
    "        if x1_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[0,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x2_train.shape[0]):\n",
    "        if x2_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[1,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x3_train.shape[0]):\n",
    "        if x3_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[2,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x4_train.shape[0]):\n",
    "        if x4_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[3,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x5_train.shape[0]):\n",
    "        if x5_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[4,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x6_train.shape[0]):\n",
    "        if x6_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[5,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x7_train.shape[0]):\n",
    "        if x7_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[6,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x8_train.shape[0]):\n",
    "        if x8_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[7,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x9_train.shape[0]):\n",
    "        if x9_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[8,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x10_train.shape[0]):\n",
    "        if x10_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[9,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x11_train.shape[0]):\n",
    "        if x11_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[10,i]=(count+1)/(100+count_words[i])\n",
    "    count=0\n",
    "    for j in range(x12_train.shape[0]):\n",
    "        if x12_train[j,i]!=0:\n",
    "            count=+1\n",
    "    prob[11,i]=(count+1)/(100+count_words[i])\n",
    "    \n",
    "print(prob[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_prior=np.zeros((12))\n",
    "prob_prior[0]=len(x1_train)/total\n",
    "prob_prior[1]=len(x2_train)/total\n",
    "prob_prior[2]=len(x3_train)/total\n",
    "prob_prior[3]=len(x4_train)/total\n",
    "prob_prior[4]=len(x5_train)/total\n",
    "prob_prior[5]=len(x6_train)/total\n",
    "prob_prior[6]=len(x7_train)/total\n",
    "prob_prior[7]=len(x8_train)/total\n",
    "prob_prior[8]=len(x9_train)/total\n",
    "prob_prior[9]=len(x10_train)/total\n",
    "prob_prior[10]=len(x11_train)/total\n",
    "prob_prior[11]=len(x12_train)/total\n",
    "print(prob_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_prob=np.ones((X_test.shape[0],12))\n",
    "for i in range(X_test.shape[0]):\n",
    "    for j in range(10000):\n",
    "        for k in range(12):\n",
    "            if X_test[i,j]!=0:\n",
    "                post_prob[i,k]=post_prob[i,k]*prob[k,j]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_prob_prior=post_prob*prob_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.zeros((X_test.shape[0]))\n",
    "for i in range(X_test.shape[0]):\n",
    "    try:\n",
    "        y_pred[i]=(np.where(post_prob[i,:] == np.amax(post_prob[i,:]))[0])+1\n",
    "    except:\n",
    "        print(i)\n",
    "        y_pred[i]=np.random.choice((np.where(post_prob[i,:] == np.amax(post_prob[i,:]))[0])+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testno=np.zeros((Y_test.size))\n",
    "for i in range(Y_test.size):\n",
    "    if Y_test[i]==\"BUSINESS\":\n",
    "        Y_testno[i]=1\n",
    "    elif Y_test[i]==\"COMEDY\":\n",
    "        Y_testno[i]=2\n",
    "    elif Y_test[i]==\"ENTERTAINMENT\":\n",
    "        Y_testno[i]=3\n",
    "    elif Y_test[i]==\"FOOD & DRINK\":\n",
    "        Y_testno[i]=4\n",
    "    elif Y_test[i]==\"HEALTHY LIVING\":\n",
    "        Y_testno[i]=5\n",
    "    elif Y_test[i]==\"PARENTING\":\n",
    "        Y_testno[i]=6\n",
    "    elif Y_test[i]==\"POLITICS\":\n",
    "        Y_testno[i]=7\n",
    "    elif Y_test[i]==\"QUEER VOICES\":\n",
    "        Y_testno[i]=8\n",
    "    elif Y_test[i]==\"SPORTS\":\n",
    "        Y_testno[i]=9\n",
    "    elif Y_test[i]==\"STYLE & BEAUTY\":\n",
    "        Y_testno[i]=10\n",
    "    elif Y_test[i]==\"TRAVEL\":\n",
    "        Y_testno[i]=11\n",
    "    elif Y_test[i]==\"WELLNESS\":\n",
    "        Y_testno[i]=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_trainno=np.zeros((Y_train.size))\n",
    "for i in range(Y_train.size):\n",
    "    if Y_train[i]==\"BUSINESS\":\n",
    "        Y_trainno[i]=1\n",
    "    elif Y_train[i]==\"COMEDY\":\n",
    "        Y_trainno[i]=2\n",
    "    elif Y_train[i]==\"ENTERTAINMENT\":\n",
    "        Y_trainno[i]=3\n",
    "    elif Y_train[i]==\"FOOD & DRINK\":\n",
    "        Y_trainno[i]=4\n",
    "    elif Y_train[i]==\"HEALTHY LIVING\":\n",
    "        Y_trainno[i]=5\n",
    "    elif Y_train[i]==\"PARENTING\":\n",
    "        Y_trainno[i]=6\n",
    "    elif Y_train[i]==\"POLITICS\":\n",
    "        Y_trainno[i]=7\n",
    "    elif Y_train[i]==\"QUEER VOICES\":\n",
    "        Y_trainno[i]=8\n",
    "    elif Y_train[i]==\"SPORTS\":\n",
    "        Y_trainno[i]=9\n",
    "    elif Y_train[i]==\"STYLE & BEAUTY\":\n",
    "        Y_trainno[i]=10\n",
    "    elif Y_train[i]==\"TRAVEL\":\n",
    "        Y_trainno[i]=11\n",
    "    elif Y_train[i]==\"WELLNESS\":\n",
    "        Y_trainno[i]=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_testno, np.asarray(y_pred)))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_testno, np.asarray(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "K={20}\n",
    "#K={3}\n",
    "count=0\n",
    "err=np.zeros((6))\n",
    "for k in K:\n",
    "    print(k)\n",
    "    y_pred=np.zeros((10000))\n",
    "    for i in range(10000):\n",
    "        neighbors=[]\n",
    "        dist=np.zeros((20000,2))\n",
    "        for j in range(20000):\n",
    "            dist[j,0]=distance.euclidean(X_train[j,:], X_test[i,:])\n",
    "            dist[j,1]=Y_trainno[j]\n",
    "        dist=dist[dist[:,0].argsort()]\n",
    "        for value in range(k):\n",
    "            neighbors.append(dist[value,1])\n",
    "        y_pred[i]=np.bincount(neighbors).argmax()\n",
    "        if Y_testno[i]!=y_pred[i]:\n",
    "            err[count]=err[count]+1\n",
    "    print(confusion_matrix(Y_testno, y_pred))\n",
    "    print(accuracy_score(Y_testno[0:10000], y_pred))\n",
    "    count=count+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost, Linear SVC, Logistic regression, bagging and decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = AdaBoostClassifier(n_estimators=20)\n",
    "#scores = cross_val_score(clf, X_train, Y_train, cv=5)\n",
    "\n",
    "y_pred = clf.fit(X_train, Y_train).predict(X_test)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(X_train,Y_train)\n",
    "Y_predict = model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test,Y_predict)*100\n",
    "print(format(accuracy, '.2f'))\n",
    "print(confusion_matrix(Y_test,Y_predict))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_Regression = LogisticRegression()\n",
    "logistic_Regression.fit(X_train,Y_train)\n",
    "Y_predict = logistic_Regression.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test,Y_predict)*100\n",
    "print(format(accuracy, '.2f'))\n",
    "print(confusion_matrix(Y_test,Y_predict))\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "model = BaggingClassifier(random_state=0, n_estimators=10)\n",
    "model.fit(X_train, Y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print('Accuracy of bagged KNN is :',accuracy_score(prediction, Y_test))\n",
    "print(confusion_matrix(Y_test,Y_predict))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "prediction_decision_tree = model.predict(X_test)\n",
    "print('The accuracy of Decision Tree is', accuracy_score(prediction_decision_tree, Y_test))\n",
    "print(confusion_matrix(Y_test,prediction_decision_tree))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
